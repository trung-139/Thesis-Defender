{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat as nbf\n",
    "import glob\n",
    "\n",
    "def py_to_nb(py_file):\n",
    "    with open(py_file) as f:\n",
    "        code = f.read()\n",
    "\n",
    "    return nbf.v4.new_code_cell(source=code)\n",
    "\n",
    "py_files = glob.glob('*.py')\n",
    "\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "for py_file in py_files:\n",
    "    new_cell = py_to_nb(py_file)\n",
    "    nb.cells.append(new_cell)\n",
    "\n",
    "with open('combined.ipynb', 'w') as f:\n",
    "    nbf.write(nb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ab5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import os\n",
    "import model as m\n",
    "import losses\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from utils import visualize_detections\n",
    "\n",
    "LABEL_MAP = {\n",
    "    1: \"No entry\",\n",
    "    2: \"No parking / waiting\",\n",
    "    3: \"No turning\",\n",
    "    4: \"Max Speed\",\n",
    "    5: \"Other prohibition signs\",\n",
    "    6: \"Warning\",\n",
    "    7: \"Mandatory\",\n",
    "}\n",
    "\n",
    "class Prediction:\n",
    "    def __init__(self,\n",
    "    inference_model,\n",
    "    crop_size=200,\n",
    "    image_height=626,\n",
    "    image_width=1622,\n",
    "    crop_height=300,\n",
    "    overlap=75,\n",
    "    dynamic_size=False,\n",
    "    tiling_size=968):\n",
    "        self.crop_size = crop_size\n",
    "        self.crop_height = crop_height\n",
    "        self.image_width = image_width\n",
    "        self.tiling_size = tiling_size\n",
    "        self.image_height = image_height\n",
    "        self.overlap = overlap\n",
    "        self.dynamic_size = dynamic_size\n",
    "        self.g_slice_indices = self.get_slice_indices(image_width)\n",
    "        self.g_slice_indices_y = self.get_slice_indices(image_height)\n",
    "        self.seperate_y = len(self.g_slice_indices_y)\n",
    "        self.inference_model = inference_model\n",
    "\n",
    "    def set_height(self, height):\n",
    "        self.image_height = height\n",
    "        self.crop_height = height // 4\n",
    "        self.g_slice_indices_y = self.get_slice_indices(height)\n",
    "\n",
    "    def set_width(self, width):\n",
    "        self.image_width = width\n",
    "        self.crop_size = width // 4\n",
    "        self.g_slice_indices = self.get_slice_indices(width)\n",
    "\n",
    "    def get_offset(self, idx):\n",
    "        cur_rank_y = idx // self.seperate\n",
    "        if idx >= self.seperate * cur_rank_y:\n",
    "            idx = idx - self.seperate * cur_rank_y\n",
    "        idx_y = cur_rank_y\n",
    "\n",
    "        return self.g_slice_indices[idx][0], self.g_slice_indices_y[idx_y][0]\n",
    "\n",
    "    def get_slice_indices(self, full_size):\n",
    "        crop_s = self.crop_size\n",
    "        over = self.overlap\n",
    "        num_paths = math.ceil(full_size / crop_s)\n",
    "\n",
    "        if full_size == self.image_height and full_size != self.image_width:\n",
    "            if self.crop_height == 0:\n",
    "                return [[0, self.image_height]]\n",
    "            crop_s = self.crop_height\n",
    "            over = 30\n",
    "        else:\n",
    "            self.seperate = num_paths\n",
    "\n",
    "        slices = []\n",
    "        for i in range(num_paths):\n",
    "            start = max(crop_s * i - over, 0)\n",
    "            end = start + crop_s\n",
    "            if end > full_size:\n",
    "                end = full_size\n",
    "                start = end - crop_s\n",
    "\n",
    "            slices.append([start, end])\n",
    "\n",
    "        return slices\n",
    "\n",
    "    def get_input_img(self, image, crop=False, crop_size=512):\n",
    "        image = tf.convert_to_tensor(image)\n",
    "\n",
    "        if self.dynamic_size:\n",
    "            shape = image.shape\n",
    "            self.set_height(shape[0])\n",
    "            self.set_width(shape[1])\n",
    "\n",
    "        train_imgs = []\n",
    "        small_imgs = []\n",
    "\n",
    "        if crop:\n",
    "            ratio = 0\n",
    "            for start_y, end_y in self.g_slice_indices_y:\n",
    "                for start_x, end_x in self.g_slice_indices:\n",
    "                    small_img = image[start_y:end_y, start_x: end_x, :]\n",
    "                    if start_x + self.crop_size > self.image_width:\n",
    "                        start_x = self.image_width -  self.crop_size\n",
    "                    if start_y + self.crop_height > self.image_height:\n",
    "                        start_y = self.image_height - self.crop_height\n",
    "                    \n",
    "                    small_img = tf.slice(image, [start_y, start_x, 0], [self.crop_height, self.crop_size, 3])\n",
    "\n",
    "                    croped, _, ratio = utils.resize_and_pad_image(small_img,\n",
    "                                                                  crop_size,\n",
    "                                                                  crop_size, jitter=None)\n",
    "                    train_imgs.append(tf.expand_dims(croped, axis=0))\n",
    "                    small_imgs.append(small_img)\n",
    "\n",
    "            return [tf.keras.applications.resnet.preprocess_input(i) for i in train_imgs], image, ratio\n",
    "\n",
    "        else:\n",
    "            train_img, _, ratio = utils.resize_and_pad_image(image,\n",
    "                                                             crop_size,\n",
    "                                                             crop_size,\n",
    "                                                             jitter=None)\n",
    "            train_img = tf.keras.applications.resnet.preprocess_input(train_img)\n",
    "            return tf.expand_dims(train_img, axis=0), image, ratio\n",
    "\n",
    "    def revert_bboxes(self, boxes, idx):\n",
    "        offset_x, offset_y = self.get_offset(idx)\n",
    "        return tf.stack([\n",
    "            boxes[idx, :, 0] + offset_x,\n",
    "            boxes[idx, :, 1] + offset_y,\n",
    "            boxes[idx, :, 2] + offset_x,\n",
    "            boxes[idx, :, 3] + offset_y,\n",
    "        ], axis=-1)\n",
    "\n",
    "    def detect_single_image(self, image, crop_sizes=[], tiling=False):\n",
    "        all_boxes = []\n",
    "        all_scores = []\n",
    "        all_classes = []\n",
    "\n",
    "        sboxes, sscores, sclasses = [], [], []\n",
    "\n",
    "        if not crop_sizes:\n",
    "            crop_sizes = [1024]\n",
    "\n",
    "        detected = False\n",
    "        if tiling:\n",
    "            input_img, image, ratio = self.get_input_img(image, crop=True, crop_size=self.tiling_size)\n",
    "\n",
    "            detections = self.inference_model.predict_on_batch(tf.concat(input_img, 0))\n",
    "\n",
    "            boxes = detections.nmsed_boxes / ratio\n",
    "            for i, valids in enumerate(detections.valid_detections):\n",
    "                if valids > 0:\n",
    "                    for j in range(valids):\n",
    "                        sboxes.append(self.revert_bboxes(boxes, i)[j])\n",
    "\n",
    "                    sclasses.append(detections.nmsed_classes[i][:valids])\n",
    "                    sscores.append(detections.nmsed_scores[i][:valids])\n",
    "\n",
    "            if len(sboxes):\n",
    "                sboxes = tf.stack(sboxes)\n",
    "                sscores = tf.concat(sscores, 0)\n",
    "                sclasses = tf.concat(sclasses, 0)\n",
    "\n",
    "        small_detections = len(sboxes)\n",
    "\n",
    "        for crop_size in crop_sizes:\n",
    "            input_img, image, ratio = self.get_input_img(image, crop=False, crop_size=crop_size)\n",
    "            detections = self.inference_model.predict(input_img)\n",
    "            num_detections = detections.valid_detections[0]\n",
    "\n",
    "            if num_detections:\n",
    "                detected = True\n",
    "                scores = detections.nmsed_scores[0][:num_detections]\n",
    "\n",
    "                all_boxes.append(detections.nmsed_boxes[0][:num_detections] / ratio)\n",
    "                all_scores.append(scores)\n",
    "                all_classes.append(detections.nmsed_classes[0][:num_detections])\n",
    "\n",
    "        if small_detections:       \n",
    "            if len(all_classes):\n",
    "                all_boxes = tf.concat(all_boxes, 0)\n",
    "                all_scores = tf.concat(all_scores, 0)\n",
    "                all_classes = tf.concat(all_classes, 0)\n",
    "\n",
    "                if detected:\n",
    "                    all_boxes = tf.concat([all_boxes, sboxes ], 0)\n",
    "                    all_scores = tf.concat([all_scores, sscores], 0)\n",
    "                    all_classes = tf.concat([all_classes, sclasses], 0)\n",
    "            else:\n",
    "                all_boxes = sboxes\n",
    "                all_scores =  sscores\n",
    "                all_classes = sclasses\n",
    "\n",
    "\n",
    "        elif detected:\n",
    "            all_boxes = tf.concat(all_boxes, 0)\n",
    "            all_scores = tf.concat(all_scores, 0)\n",
    "            all_classes = tf.concat(all_classes, 0)\n",
    "\n",
    "        if detected or small_detections:\n",
    "            selected_indices = tf.image.non_max_suppression(\n",
    "                all_boxes,\n",
    "                all_scores,\n",
    "                50,\n",
    "                iou_threshold=0.1,\n",
    "                score_threshold=0.5,\n",
    "            )\n",
    "\n",
    "            selected_indices = selected_indices.numpy()\n",
    "\n",
    "            if len(selected_indices):\n",
    "                return (image,\n",
    "                        tf.gather(all_boxes, selected_indices),\n",
    "                        tf.gather(all_scores, selected_indices),\n",
    "                        tf.gather(all_classes, selected_indices))\n",
    "\n",
    "        return image, all_boxes, all_scores, all_classes\n",
    "\n",
    "\n",
    "def get_inference_model(weight_path, backbone=\"resnet50\"):\n",
    "    num_of_classes = 7\n",
    "    model = m.RetinaNet(num_of_classes, backbone=backbone)\n",
    "    model.compile(optimizer=\"adam\", loss=losses.RetinaNetLoss(num_of_classes))\n",
    "    model.build((1, None, None, 3))\n",
    "    image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "    model.load_weights(weight_path)\n",
    "    predictions = model(image, training=False)\n",
    "    detections = m.DecodePredictions(confidence_threshold=0.5,\n",
    "                                     num_classes=num_of_classes,\n",
    "                                     max_detections_per_class=10,\n",
    "                                     nms_iou_threshold=0.5,\n",
    "                                     verbose=0)(image, predictions)\n",
    "\n",
    "    inference_model = tf.keras.Model(inputs=image, outputs=detections)\n",
    "\n",
    "    return inference_model\n",
    "\n",
    "def combine_prediction(\n",
    "    prediction_1,\n",
    "    prediction_2,\n",
    "    weight_1=1,\n",
    "    max_detections=50,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.65):\n",
    "    boxes_1, scores_1, classes_1 = prediction_1\n",
    "    boxes_2, scores_2, classes_2 = prediction_2\n",
    "\n",
    "    weight_2 = 1 - weight_1\n",
    "    highest = max(weight_1, weight_2)\n",
    "    score_threshold *= highest\n",
    "    \n",
    "    if not len(scores_1) and len(scores_2):\n",
    "        scores = scores_2 * weight_2\n",
    "        boxes = boxes_2\n",
    "        classes = classes_2\n",
    "    elif not len(scores_2) and len(scores_1):\n",
    "        scores = scores_1 * weight_1\n",
    "        boxes = boxes_1\n",
    "        classes = classes_1\n",
    "\n",
    "    elif not len(scores_1) and not len(scores_2):\n",
    "        return boxes_1, scores_1, classes_1\n",
    "\n",
    "    else:\n",
    "        scores_1 *= weight_1\n",
    "        scores_2 *= weight_2\n",
    "\n",
    "        boxes = tf.concat([boxes_1, boxes_2], 0)\n",
    "        scores = tf.concat([scores_1, scores_2], 0)\n",
    "        classes = tf.concat([classes_1, classes_2], 0)\n",
    "\n",
    "    selected_indices =  tf.image.non_max_suppression(\n",
    "        boxes,\n",
    "        scores,\n",
    "        max_detections,\n",
    "        iou_threshold=iou_threshold,\n",
    "        score_threshold=score_threshold,\n",
    "    )\n",
    "\n",
    "    return (tf.gather(boxes, selected_indices),\n",
    "            tf.gather(scores / highest, selected_indices),\n",
    "            tf.gather(classes, selected_indices))\n",
    "\n",
    "\n",
    "def run_prediction(args):\n",
    "    input_path, output_path, weight, save_dir = (\n",
    "        args.input, args.output, args.weight, args.save_dir)\n",
    "\n",
    "    backbone = weight.split(\"_\")[-1].replace(\".h5\", \"\")\n",
    "    crop_sizes = list(map(int, args.scales.split(\",\")))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(\"/\".join(output_path.split(\"/\")[:-1]), exist_ok=True)\n",
    "\n",
    "    if output_path.split(\".\")[-1] != \"json\":\n",
    "        raise ValueError(\"Output file should be json format\")\n",
    "\n",
    "    # Get list of test images\n",
    "    if os.path.isdir(input_path):\n",
    "        image_files = glob.glob(os.path.join(input_path, '*'))\n",
    "    else:\n",
    "        # it's file\n",
    "        image_files = [input_path]\n",
    "\n",
    "    print(f\"Test on {len(image_files)} images\")\n",
    "\n",
    "    # Create submission.json\n",
    "    submission = []\n",
    "    predictor = Prediction(get_inference_model(weight, backbone))\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    for file_path in tqdm(image_files):\n",
    "        image, boxes, scores, classes = predictor.detect_single_image(\n",
    "            cv2.imread(file_path)[..., ::-1],\n",
    "            crop_sizes=crop_sizes,\n",
    "            tiling=args.tiling\n",
    "        )\n",
    "        if not isinstance(boxes, list):\n",
    "            boxes = boxes.numpy()\n",
    "            scores = scores.numpy()\n",
    "            classes = classes.numpy()\n",
    "\n",
    "        if save_dir:\n",
    "            save_path = os.path.join(save_dir, file_path.split(\"/\")[-1])\n",
    "            cls_name = [\n",
    "                LABEL_MAP[int(x)] for x in classes\n",
    "            ]\n",
    "            visualize_detections(image, boxes, cls_name, scores, save_path=save_path)\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            x1, y1, x2, y2 = box\n",
    "            xywh = [x1, y1, x2 - x1, y2 - y1]\n",
    "            score = scores[i]\n",
    "            cls = classes[i]\n",
    "            submission.append({\n",
    "                \"image_id\": file_path,\n",
    "                \"category_id\": int(cls),\n",
    "                \"bbox\": [float(z) for z in xywh],\n",
    "                \"score\": float(score),\n",
    "            })\n",
    "\n",
    "    print(\"Predict in {}\".format(datetime.datetime.now() - start))\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "\n",
    "    print(\"Submission saved at {}\".format(output_path))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Traffic sign detection')\n",
    "    parser.add_argument(\"--input\",\n",
    "                        metavar=\"I\", type=str, default=\"/data/images\",\n",
    "                        help=\"Path to input images\")\n",
    "    parser.add_argument(\"--output\", metavar=\"O\", type=str,\n",
    "                        default=\"/data/result/submission.json\", help=\"Output file path\")\n",
    "    parser.add_argument(\"--weight\", metavar=\"W\", type=str,\n",
    "                        default=\"pretrained_densenet121\", help=\"Weight path\")\n",
    "    parser.add_argument(\"--save-dir\", type=str, default=\"/content/infernece_images\")\n",
    "    parser.add_argument(\"--tiling\", action=\"store_true\")\n",
    "    parser.add_argument(\"--scales\", type=str, default=\"1024\", help=\"Separated by comma ','\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    run_prediction(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from utils import convert_to_corners, compute_iou\n",
    "from data_processing import resize_and_pad_image\n",
    "from tensorflow import keras\n",
    "\n",
    "def get_backbone(name=\"resnet50\", weight=None):\n",
    "    \"\"\"Supported backbone: resnet50, resnet101, densenet121\"\"\"\n",
    "    backbone = None\n",
    "    if \"resnet\" in name:\n",
    "        if name == \"resnet50\":\n",
    "            backbone = keras.applications.ResNet50\n",
    "        elif name == \"resnet101\":\n",
    "            backbone = keras.applications.ResNet101\n",
    "\n",
    "        output_layers = [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
    "\n",
    "    elif \"densenet\" in name:\n",
    "        if name == \"densenet121\":\n",
    "            backbone = keras.applications.DenseNet121\n",
    "            output_layers = [\"pool3_conv\", \"pool4_conv\", \"relu\"]\n",
    "\n",
    "    backbone_model = backbone(include_top=False, input_shape=[None, None, 3], weights=weight)\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone_model.get_layer(layer_name).output\n",
    "        for layer_name in output_layers\n",
    "    ]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone_model.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "\n",
    "\n",
    "class FeaturePyramid(keras.layers.Layer):\n",
    "    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset.\n",
    "      backbone: The backbone to build the feature pyramid from.\n",
    "        Currently supports ResNet50 only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone=\"resnet50\", weight=None, **kwargs):\n",
    "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
    "        self.backbone = get_backbone(backbone, weight)\n",
    "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
    "        p3_output = self.conv_c3_1x1(c3_output)\n",
    "        p4_output = self.conv_c4_1x1(c4_output)\n",
    "        p5_output = self.conv_c5_1x1(c5_output)\n",
    "        p4_output = p4_output + keras.layers.UpSampling2D(2)(p5_output)\n",
    "        p3_output = p3_output + keras.layers.UpSampling2D(2)(p4_output)\n",
    "        p3_output = self.conv_c3_3x3(p3_output)\n",
    "        p4_output = self.conv_c4_3x3(p4_output)\n",
    "        p5_output = self.conv_c5_3x3(p5_output)\n",
    "        p6_output = self.conv_c6_3x3(c5_output)\n",
    "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "        return p3_output, p4_output, p5_output, p6_output, p7_output\n",
    "\n",
    "\n",
    "def build_head(output_filters, bias_init):\n",
    "    \"\"\"Builds the class/box predictions head.\n",
    "\n",
    "    Arguments:\n",
    "      output_filters: Number of convolution filters in the final layer.\n",
    "      bias_init: Bias Initializer for the final convolution layer.\n",
    "\n",
    "    Returns:\n",
    "      A keras sequential model representing either the classification\n",
    "        or the box regression head depending on `output_filters`.\n",
    "    \"\"\"\n",
    "    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
    "    for _ in range(4):\n",
    "        head.add(\n",
    "            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
    "        )\n",
    "        head.add(keras.layers.ReLU())\n",
    "    head.add(\n",
    "        keras.layers.Conv2D(\n",
    "            output_filters,\n",
    "            3,\n",
    "            1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            bias_initializer=bias_init,\n",
    "        )\n",
    "    )\n",
    "    return head\n",
    "\n",
    "\n",
    "class RetinaNet(keras.Model):\n",
    "    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset.\n",
    "      backbone: The backbone to build the feature pyramid from.\n",
    "        Currently supports ResNet50 only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, backbone=None, weight=None, **kwargs):\n",
    "        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n",
    "        self.backbone_name = backbone\n",
    "        self.fpn = FeaturePyramid(backbone, weight)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(9 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=False):\n",
    "        features = self.fpn(image, training=True)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "        for feature in features:\n",
    "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
    "            cls_outputs.append(\n",
    "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
    "            )\n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        return tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(3, 8)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(3, 8)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "\n",
    "\n",
    "class DecodePredictions(tf.keras.layers.Layer):\n",
    "    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset\n",
    "      confidence_threshold: Minimum class probability, below which detections\n",
    "        are pruned.\n",
    "      nms_iou_threshold: IOU threshold for the NMS operation\n",
    "      max_detections_per_class: Maximum number of detections to retain per\n",
    "       class.\n",
    "      max_detections: Maximum number of detections to retain across all\n",
    "        classes.\n",
    "      box_variance: The scaling factors used to scale the bounding box\n",
    "        predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=80,\n",
    "        confidence_threshold=0.05,\n",
    "        nms_iou_threshold=0.5,\n",
    "        max_detections_per_class=100,\n",
    "        max_detections=100,\n",
    "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
    "        verbose=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(DecodePredictions, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.verbose = verbose\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.nms_iou_threshold = nms_iou_threshold\n",
    "        self.max_detections_per_class = max_detections_per_class\n",
    "        self.max_detections = max_detections\n",
    "\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
    "        boxes = box_predictions * self._box_variance\n",
    "        boxes = tf.concat(\n",
    "            [\n",
    "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        boxes_transformed = convert_to_corners(boxes)\n",
    "        return boxes_transformed\n",
    "\n",
    "    def call(self, images, predictions):\n",
    "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        box_predictions = predictions[:, :, :4]\n",
    "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
    "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "        return tf.image.combined_non_max_suppression(\n",
    "            tf.expand_dims(boxes, axis=2),\n",
    "            cls_predictions,\n",
    "            self.max_detections_per_class,\n",
    "            self.max_detections,\n",
    "            self.nms_iou_threshold,\n",
    "            self.confidence_threshold,\n",
    "            clip_boxes=False,\n",
    "        )\n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
    "    ):\n",
    "        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n",
    "\n",
    "        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n",
    "          to get a `(M, N)` shaped matrix.\n",
    "        2. The ground truth box with the maximum IOU in each row is assigned to\n",
    "          the anchor box provided the IOU is greater than `match_iou`.\n",
    "        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n",
    "          box is assigned with the background class.\n",
    "        4. The remaining anchor boxes that do not have any class assigned are\n",
    "          ignored during training.\n",
    "\n",
    "        Arguments:\n",
    "          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n",
    "            representing all the anchor boxes for a given input image shape,\n",
    "            where each anchor box is of the format `[x, y, width, height]`.\n",
    "          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n",
    "            the ground truth boxes, where each box is of the format\n",
    "            `[x, y, width, height]`.\n",
    "          match_iou: A float value representing the minimum IOU threshold for\n",
    "            determining if a ground truth box can be assigned to an anchor box.\n",
    "          ignore_iou: A float value representing the IOU threshold under which\n",
    "            an anchor box is assigned to the background class.\n",
    "\n",
    "        Returns:\n",
    "          matched_gt_idx: Index of the matched object\n",
    "          positive_mask: A mask for anchor boxes that have been assigned ground\n",
    "            truth boxes.\n",
    "          ignore_mask: A mask for anchor boxes that need to by ignored during\n",
    "            training\n",
    "        \"\"\"\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
    "        )\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            labels = labels.write(i, label)\n",
    "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
    "        return batch_images, labels.stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "\n",
    "def image_resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def get_img(path, width=None):\n",
    "    img = cv2.imread(path)\n",
    "    if width:\n",
    "        img = image_resize(img, width=width)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image,\n",
    "    min_side=512,\n",
    "    max_side=1024,\n",
    "    jitter=[512, 1024],\n",
    "    stride=128.0\n",
    "):\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(\n",
    "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
    "    )\n",
    "    return image, image_shape, ratio\n",
    "\n",
    "\n",
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def to_xyxy(bbox):\n",
    "    return tf.stack(\n",
    "        [bbox[:, 0], bbox[:, 1], bbox[:, 2] + bbox[:, 0], bbox[:, 3] + bbox[:, 1],],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_bbox(bbox, w=1622, h=626):\n",
    "    return tf.stack([\n",
    "        bbox[:, 0] / w,\n",
    "        bbox[:, 1] / h,\n",
    "        bbox[:, 2] / w,\n",
    "        bbox[:, 3] / h,\n",
    "    ], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_iou(boxes1, boxes2):\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(15, 15), linewidth=2, color=[1, 0, 0],\n",
    "    box_true=None, label_true=None, save_path=''\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for i in range(len(boxes)):\n",
    "        box, _cls, score = boxes[i], classes[i], scores[i]\n",
    "\n",
    "        text = \"{}: {:.2f}\".format(_cls, score)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        ax.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            text,\n",
    "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
    "            clip_box=ax.clipbox,\n",
    "            clip_on=True,\n",
    "        )\n",
    "\n",
    "    if box_true is not None and label_true is not None:\n",
    "        for i in range(len(box_true)):\n",
    "            box_t, cls_t = box_true[i], label_true[i]\n",
    "            text = \"{}: {:.2f}\".format(cls_t, 1.0)\n",
    "            x1, y1, w, h = box_t\n",
    "            patch = plt.Rectangle(\n",
    "                [x1, y1], w, h, fill=False,\n",
    "                edgecolor=[1,1,1], linewidth=3\n",
    "            )\n",
    "            ax.add_patch(patch)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def try_ignore_error(func, *argv):\n",
    "    \"\"\"\n",
    "    Try and ignore error\n",
    "    @params:\n",
    "      + func: function\n",
    "      + *argv: arguments of func\n",
    "    \"\"\"\n",
    "    try:\n",
    "        func(*argv)\n",
    "    except Exception as e:\n",
    "        print(\"WARN: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genericpath import exists\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import model as m\n",
    "import data_processing\n",
    "import losses\n",
    "import utils\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Traffic sign detection')\n",
    "    parser.add_argument(\"--input\", dest=\"input_path\",\n",
    "                        metavar=\"I\", type=str, default=\"/data/images\",\n",
    "                        help=\"Path to training images\")\n",
    "    parser.add_argument(\"--backbone\", type=str, default='resnet50')\n",
    "    parser.add_argument(\"--init-from\", type=str, default='resnet50',\n",
    "                        help='Path to pretrained weight or backbone name')\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=2)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--n-classes\", type=int, default=7)\n",
    "    parser.add_argument(\"--checkpoint-dir\", type=str, default='weights')\n",
    "    parser.add_argument(\"--force-tfrec\", action='store_true')\n",
    "    parser.add_argument(\"--debug-samples\", type=int, default=0)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main(args):\n",
    "    TFRECORDS_FILE = \"/tmp/images.tfrecords\"\n",
    "    metadata = json.load(open(\"./train_traffic_sign_dataset.json\", \"r\"))\n",
    "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    if args.force_tfrec or not os.path.isfile(TFRECORDS_FILE):\n",
    "        print(\"Create tfrecords dataset\")\n",
    "        data_processing.write_tfrecords(\n",
    "            data_processing.create_dataset_list(metadata[\"annotations\"]),\n",
    "            TFRECORDS_FILE,\n",
    "             args.input_path\n",
    "        )\n",
    "\n",
    "    autotune = tf.data.experimental.AUTOTUNE\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "\n",
    "    fdataset = tf.data.TFRecordDataset(TFRECORDS_FILE)\n",
    "    data_processor = data_processing.DataProcessing(width=400, height=154)\n",
    "    label_encoder = m.LabelEncoder()\n",
    "    dataset = fdataset.map(data_processor.preprocess_data)\n",
    "    dataset = dataset.shuffle(batch_size)\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padding_values=(0.0, 1e-8, tf.cast(-1, tf.int64)),\n",
    "        drop_remainder=True,\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        label_encoder.encode_batch, num_parallel_calls=autotune\n",
    "    )\n",
    "    dataset = dataset.apply(tf.data.experimental.ignore_errors())\n",
    "    dataset = dataset.prefetch(autotune)\n",
    "\n",
    "    train_size = args.debug_samples or 4500\n",
    "    train_data = dataset\n",
    "    train_steps_per_epoch = train_size // batch_size\n",
    "    train_steps = 6 * 10000\n",
    "    epochs = train_steps // train_steps_per_epoch\n",
    "\n",
    "    learning_rates = [1e-4, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "    learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "    learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "        boundaries=learning_rate_boundaries, values=learning_rates\n",
    "    )\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(args.checkpoint_dir, f'weight_{args.backbone}.h5'),\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = m.RetinaNet(args.n_classes, backbone=args.backbone)\n",
    "    model.compile(optimizer=optimizer, loss=losses.RetinaNetLoss(args.n_classes))\n",
    "    model.build((1, None, None, 3))\n",
    "    utils.try_ignore_error(model.load_weights, args.init_from)\n",
    "\n",
    "    H = model.fit(train_data.repeat(),\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=train_steps_per_epoch,\n",
    "                callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data_augmentation as augmentation\n",
    "from utils import (\n",
    "    resize_and_pad_image,\n",
    "    swap_xy,\n",
    "    convert_to_xywh,\n",
    "    convert_to_corners,\n",
    "    to_xyxy,\n",
    "    normalize_bbox,\n",
    ")\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_feature_description = {\n",
    "    \"bbox\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def float_array_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def has_small_bbox(bboxes):\n",
    "    areas = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])\n",
    "    min_val = tf.constant(650, dtype=tf.float32)\n",
    "    return tf.math.reduce_any(tf.math.less(areas, min_val))\n",
    "\n",
    "def create_dataset_list(annotations):\n",
    "    image_bboxes = {}\n",
    "    for item in annotations:\n",
    "        img_id = item.get(\"image_id\")\n",
    "        if img_id in image_bboxes:\n",
    "            image_bboxes[img_id][\"bbox\"].append(item[\"bbox\"])\n",
    "            image_bboxes[img_id][\"label\"].append(item[\"category_id\"])\n",
    "        else:\n",
    "            image_bboxes[img_id] = {\n",
    "                \"id\": img_id,\n",
    "                \"bbox\": [item[\"bbox\"]],\n",
    "                \"label\": [item[\"category_id\"]],\n",
    "            }\n",
    "\n",
    "    return list(image_bboxes.values())\n",
    "\n",
    "\n",
    "def image_example(image_string, label, bbox):\n",
    "    feature = {\n",
    "        \"bbox\": bytes_feature(bbox),\n",
    "        \"label\": bytes_feature(label),\n",
    "        \"image\": bytes_feature(image_string),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(data, file_path, train_dir):\n",
    "    if train_dir.endswith(\"images\"):\n",
    "        train_dir = train_dir.replace(\"images\", \"\")\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_path) as writer:\n",
    "        for img_info in tqdm(data):\n",
    "            ipath = \"{}images/{}.png\".format(train_dir, img_info[\"id\"])\n",
    "            image_string = open(ipath, \"rb\").read()\n",
    "            tf_example = image_example(\n",
    "                image_string,\n",
    "                np.array(img_info[\"label\"]).tobytes(),\n",
    "                np.array(img_info[\"bbox\"]).tobytes(),\n",
    "            )\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "class DataProcessing:\n",
    "    def __init__(self, origin_width=1622, origin_height=626 , width=400,\n",
    "                height=154, augment=True, mix_iterator=None,convert_xywh=True,\n",
    "                random_cropping=True, dynamic_size=False):\n",
    "        self.origin_width = origin_width\n",
    "        self.origin_height = origin_height\n",
    "        self.dynamic_size = dynamic_size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.random_cropping = random_cropping\n",
    "        self.scale_x = self.origin_width / self.width\n",
    "        self.scale_y = self.origin_height / self.height\n",
    "        self.convert_xywh = convert_xywh\n",
    "        self.augment = augment\n",
    "        self.mix_iterator = mix_iterator\n",
    "\n",
    "    def set_width(self, width):\n",
    "        self.width = width\n",
    "\n",
    "    def set_height(self, height):\n",
    "        self.height = height\n",
    "\n",
    "    def moved_box(self, box, x1, y1, width, height):\n",
    "        x1, y1 = tf.cast(x1, tf.float32), tf.cast(y1, tf.float32)\n",
    "\n",
    "        return tf.stack([\n",
    "            (box[:, 0] - x1) * self.scale_x,\n",
    "            (box[:, 1] - y1) * self.scale_y,\n",
    "            (box[:, 2] - x1) * self.scale_x,\n",
    "            (box[:, 3] - y1) * self.scale_y,\n",
    "        ], axis=1)\n",
    "\n",
    "\n",
    "    def get_slice_indices(self):\n",
    "        num_paths = math.ceil(self.origin_width / self.width)\n",
    "        slices = []\n",
    "        for i in range(num_paths):\n",
    "            start = max(self.width * i - self.overlap_x, 0)\n",
    "            end = start + self.width\n",
    "            if end > self.origin_width:\n",
    "                start = end - self.origin_width\n",
    "                end = self.origin_width\n",
    "\n",
    "            slices.append([start, end])\n",
    "\n",
    "        return slices\n",
    "\n",
    "    def random_crop(self, image, bbox, labels):\n",
    "        width = self.width\n",
    "        height = self.height\n",
    "        idx = tf.random.uniform((), 0, tf.shape(bbox)[0], tf.int32)\n",
    "        selected_box = bbox[idx]\n",
    "        x1, y1, x2, y2 = tf.unstack(selected_box, axis=0)\n",
    "        x1 = tf.cast(x1, tf.int32)\n",
    "        x2 = tf.cast(x2, tf.int32)\n",
    "        y1 = tf.cast(y1, tf.int32)\n",
    "        y2 = tf.cast(y2, tf.int32)\n",
    "\n",
    "        # 60% part of object lie inside the frame is considered valid\n",
    "        accept_ratio = 0.6\n",
    "        mean_x1, mean_x2 = tf.reduce_mean(bbox[:, 0]), tf.reduce_mean(bbox[:, 2])\n",
    "        pad_size = accept_ratio * (mean_x2 - mean_x1)\n",
    "\n",
    "        x1 = tf.random.uniform((), x1 - width, x1, dtype=tf.int32)\n",
    "        y1 = tf.random.uniform((), y1 - height, y1, dtype=tf.int32)\n",
    "\n",
    "        if tf.less(x1, 0):\n",
    "            x1 = 0\n",
    "\n",
    "        if tf.less(y1, 0):\n",
    "            y1 = 0\n",
    "\n",
    "        if tf.greater(x1 + width, self.origin_width):\n",
    "            x1 = self.origin_width - width\n",
    "\n",
    "        if tf.greater(y1 + height, self.origin_height):\n",
    "            y1 = self.origin_height - height\n",
    "\n",
    "        if tf.greater(y2, y1 + height):\n",
    "            y1 = y1 + (y2 - (y1 + height))\n",
    "\n",
    "        if tf.greater(x2, x1 + width):\n",
    "            x1 = x1 + (x2 - (x1 + width))\n",
    "\n",
    "        # [height, width, channels]\n",
    "        cropped = tf.slice(image, [y1, x1, 0], [height, width, 3])\n",
    "\n",
    "        x1 = tf.cast(x1, tf.float32)\n",
    "        y1 = tf.cast(y1, tf.float32)\n",
    "        width = tf.cast(width, tf.float32)\n",
    "        height = tf.cast(height, tf.float32)\n",
    "\n",
    "        # filter out boxes that not lie inside the cropped image\n",
    "        x1_b, y1_b, x2_b, y2_b = tf.unstack(bbox, axis=1)\n",
    "\n",
    "        # 1. x1 of box > cropped width\n",
    "        # 2. x2 of box < cropped width\n",
    "        x_condition = tf.logical_and(\n",
    "            tf.greater(x1_b, x1 - pad_size),\n",
    "            tf.less(x2_b, x1 + width + pad_size)\n",
    "        )\n",
    "        # 3. y1 of box> cropped height\n",
    "        # 4. y2 of box> cropped height\n",
    "        y_condition = tf.logical_and(\n",
    "            tf.greater(y1_b, y1 - pad_size),\n",
    "            tf.less(y2_b, y1 + height + pad_size)\n",
    "        )\n",
    "\n",
    "        cond = tf.logical_and(x_condition, y_condition)\n",
    "        positive_mask = tf.where(cond)\n",
    "\n",
    "        bbox = self.moved_box(bbox, x1, y1, width, height)\n",
    "        bbox = tf.gather_nd(bbox, positive_mask)\n",
    "        labels = tf.gather_nd(labels, positive_mask)\n",
    "\n",
    "        return cropped, bbox, labels\n",
    "\n",
    "    def preprocess_data(self, example):\n",
    "        \"\"\"\n",
    "        Applies preprocessing step to a single example\n",
    "        \"\"\"\n",
    "        sample = tf.io.parse_single_example(example, image_feature_description)\n",
    "        image = tf.image.decode_png(sample[\"image\"])\n",
    "        bbox = tf.cast(\n",
    "            tf.io.decode_raw(sample[\"bbox\"], out_type=tf.int64), dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        label = tf.io.decode_raw(sample[\"label\"], out_type=tf.int64)\n",
    "        bbox = to_xyxy(tf.reshape(bbox, (-1, 4)))\n",
    "\n",
    "        if self.dynamic_size:\n",
    "            shape = tf.shape(image)\n",
    "            self.origin_width = shape[1]\n",
    "            self.origin_height = shape[0]\n",
    "\n",
    "        if not self.augment:\n",
    "            image, bbox, label = self.random_crop(image, bbox, label)\n",
    "            image = tf.image.resize(image, (self.origin_height, self.origin_width))\n",
    "            if self.convert_xywh:\n",
    "                bbox = convert_to_xywh(bbox)\n",
    "            return image, bbox, label\n",
    "\n",
    "        # Data augmentation\n",
    "        image = augmentation.random_adjust_brightness(image)\n",
    "        image = augmentation.random_adjust_contrast(image)\n",
    "        # crop the region contain at least 1 bounding box\n",
    "        has_smallb = has_small_bbox(bbox)\n",
    "        if self.random_cropping and tf.logical_or(has_smallb, tf.random.uniform(()) > 0.5):\n",
    "            image, bbox, label = self.random_crop(image, bbox, label)\n",
    "\n",
    "        bbox = normalize_bbox(bbox, self.origin_width, self.origin_height)\n",
    "        image, bbox = augmentation.random_flip_horizontal(image, bbox)\n",
    "\n",
    "        if not has_smallb:\n",
    "            image = augmentation.random_gaussian_blur(image, 0.5)\n",
    "\n",
    "        image, image_shape, _ = resize_and_pad_image(image, jitter=None)\n",
    "        w, h = image_shape[0], image_shape[1]\n",
    "\n",
    "        bbox = tf.stack([\n",
    "            bbox[:, 0] * h,\n",
    "            bbox[:, 1] * w,\n",
    "            bbox[:, 2] * h,\n",
    "            bbox[:, 3] * w,\n",
    "        ], axis=-1)\n",
    "\n",
    "        if self.convert_xywh:\n",
    "            bbox = convert_to_xywh(bbox)\n",
    "\n",
    "        return image, bbox, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma, label_smoothing):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._label_smoothing = label_smoothing\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred\n",
    "        )\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "\n",
    "        if self._label_smoothing:\n",
    "            alpha = tf.where(tf.greater(y_true, 0.91), self._alpha, (1.0 - self._alpha))\n",
    "            pt = tf.where(tf.greater(y_true, 0.91), probs, 1 - probs)\n",
    "        else:\n",
    "            alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "            pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.25,\n",
    "                gamma=2.0, delta=1.0, label_smoothing=False,\n",
    "                ):\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma, label_smoothing)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "        self._label_smoothing = label_smoothing\n",
    "        self._factor = 0.1\n",
    "        self._max_label = (1 - self._factor)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        if self._label_smoothing:\n",
    "            cls_labels = _smooth_labels(cls_labels)\n",
    "\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "\n",
    "        if self._label_smoothing:\n",
    "            clf_loss = tf.where(tf.greater(ignore_mask, 0.8), 0.0, clf_loss)\n",
    "            box_loss = tf.where(tf.equal(positive_mask, 1), box_loss, 0.0)\n",
    "        else:\n",
    "            clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
    "            box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "\n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "\n",
    "        loss = clf_loss + box_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _smooth_labels(labels):\n",
    "    \"\"\"Apply label smoothing\"\"\"\n",
    "    factor = 0.1\n",
    "    labels = labels * (1 - factor)\n",
    "    labels = labels + (factor / tf.cast(tf.shape(labels)[1], tf.float32))\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "size = 3\n",
    "kernel_motion_blur = np.zeros((size, size))\n",
    "kernel_motion_blur[int((size - 1) / 2), :] = np.ones(size)\n",
    "kernel_motion_blur = kernel_motion_blur / size\n",
    "kernel_motion_blur = np.expand_dims(kernel_motion_blur, axis=-1)\n",
    "kernel_motion_blur = np.repeat(kernel_motion_blur, repeats=3, axis=-1)\n",
    "kernel_motion_blur = np.expand_dims(kernel_motion_blur, axis=-1)\n",
    "kernel_motion_blur = tf.cast(kernel_motion_blur, tf.float32)\n",
    "\n",
    "\n",
    "def random_flip_horizontal(image, boxes, prob=0.5):\n",
    "    if tf.random.uniform(()) > prob:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack(\n",
    "            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
    "        )\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def random_adjust_contrast(image, prob=0.5):\n",
    "    if tf.random.uniform(()) > prob:\n",
    "        factor = tf.random.uniform((), 0.5, 2.0)\n",
    "        return tf.image.adjust_contrast(image, factor)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_adjust_brightness(image, prob=0.5):\n",
    "    if tf.random.uniform(()) > prob:\n",
    "        return tf.image.random_brightness(image, 0.06)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def _gaussian_kernel(kernel_size, sigma, n_channels, dtype):\n",
    "    x = tf.range(-kernel_size // 2 + 1, kernel_size // 2 + 1, dtype=dtype)\n",
    "    g = tf.math.exp(-(tf.pow(x, 2) / (2 * tf.pow(tf.cast(sigma, dtype), 2))))\n",
    "    g_norm2d = tf.pow(tf.reduce_sum(g), 2)\n",
    "    g_kernel = tf.tensordot(g, g, axes=0) / g_norm2d\n",
    "    g_kernel = tf.expand_dims(g_kernel, axis=-1)\n",
    "    return tf.expand_dims(tf.tile(g_kernel, (1, 1, n_channels)), axis=-1)\n",
    "\n",
    "\n",
    "def random_gaussian_blur(img, prob=0.9):\n",
    "    if tf.random.uniform(()) > prob:\n",
    "        img = tf.cast(img, dtype=tf.float32)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            kernel = _gaussian_kernel(7, 3, 3, img.dtype)\n",
    "        else:\n",
    "            kernel = kernel_motion_blur\n",
    "        img = tf.nn.depthwise_conv2d(img[None], kernel, [1, 1, 1, 1], \"SAME\")\n",
    "\n",
    "        return tf.cast(img[0], dtype=tf.uint8)\n",
    "\n",
    "    return img\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
